/* eslint-disable no-console */
import type { CrawlStrategy } from './lib/strategy'
import process from 'node:process'
import { BaseCrawler } from './lib/base-crawler'
import { Site92Hm } from './strategies/site-92hm'
import { SiteSe8 } from './strategies/site-se8'
import 'dotenv/config'

class Runner extends BaseCrawler {
  private strategies: CrawlStrategy[] = [
    new Site92Hm(),
    new SiteSe8(),
  ]

  private queue: string[] = []
  private visited = new Set<string>()
  private MAX_PAGES = 100000
  private processedCount = 0
  private activeWorkers = 0
  private CONCURRENCY = Number(process.env.CONCURRENCY) || 20

  async run() {
    const startUrl = process.argv[2]

    if (!startUrl) {
      console.warn('âš ï¸  Please provide a target URL or "full" as an argument.')
      console.log('Example: pnpm start https://www.92hm.life/booklist?end=0')
      console.log('Example: pnpm start full')
      return
    }

    await this.initBrowser()

    try {
      if (startUrl === 'full') {
        console.log('ğŸš€ Starting Full Scan Mode...')
        this.MAX_PAGES = 50000 // å¢åŠ çˆ¬å–ä¸Šé™

        const tags = [
          'é’æ˜¥',
          'æ€§æ„Ÿ',
          'é•¿è…¿',
          'å¤šäºº',
          'å¾¡å§',
          'å·¨ä¹³',
          'æ–°å©š',
          'åª³å¦‡',
          'æš§æ˜§',
          'æ¸…çº¯',
          'è°ƒæ•™',
          'å°‘å¦‡',
          'é£éªš',
          'åŒå±…',
          'æ·«ä¹±',
          'å¥½å‹',
          'å¥³ç¥',
          'è¯±æƒ‘',
          'å·æƒ…',
          'å‡ºè½¨',
          'æ­£å¦¹',
          'å®¶æ•™',
        ]
        const areas = [1, 2, 3] // 1:éŸ©å›½, 2:æ—¥æœ¬, 3:å°æ¹¾
        const ends = [0, 1] // 0:è¿è½½, 1:å®Œç»“

        for (const tag of tags) {
          for (const area of areas) {
            for (const end of ends) {
              const url = `https://www.92hm.life/booklist?tag=${encodeURIComponent(tag)}&area=${area}&end=${end}`
              this.queue.push(url)
            }
          }
        }
        console.log(`âœ… Generated ${this.queue.length} seed URLs.`)
      }
      else {
        this.queue.push(startUrl)
      }

      // å¯åŠ¨ Worker Pool
      const workers = Array.from({ length: this.CONCURRENCY }).fill(null).map((_, i) => this.worker(i + 1))
      await Promise.all(workers)

      console.log('ğŸ‰ All tasks completed!')
    }
    finally {
      await this.closeBrowser()
    }
  }

  private async worker(id: number) {
    console.log(`ğŸ‘· Worker ${id} started`)

    while (this.processedCount < this.MAX_PAGES) {
      const url = this.queue.shift()

      // é˜Ÿåˆ—ä¸ºç©ºï¼Œä¸”æ²¡æœ‰å…¶ä»– Worker åœ¨å·¥ä½œï¼ˆè¯´æ˜çœŸçš„æ²¡ä»»åŠ¡äº†ï¼‰ï¼Œåˆ™é€€å‡º
      if (!url) {
        if (this.activeWorkers === 0 && this.queue.length === 0) {
          break
        }
        // å¦‚æœé˜Ÿåˆ—æš‚æ—¶ä¸ºç©ºä½†å…¶ä»– Worker è¿˜åœ¨è·‘ï¼Œå¯èƒ½äº§ç”Ÿæ–°è¿æ¥ï¼Œç¨å¾®ç­‰å¾…
        await new Promise(r => setTimeout(r, 1000))
        continue
      }

      if (this.visited.has(url))
        continue
      this.visited.add(url)

      this.activeWorkers++
      this.processedCount++
      console.log(`[Worker ${id}] [${this.processedCount}/${this.MAX_PAGES}] Processing: ${url} (Queue: ${this.queue.length})`)

      try {
        await this.processUrl(url)
      }
      catch (e) {
        console.error(`[Worker ${id}] âŒ Error processing ${url}:`, e)
      }
      finally {
        this.activeWorkers--
        // éšæœºå»¶è¿Ÿé˜²æ­¢å°ç¦
        const delay = Math.floor(Math.random() * 500) + 500
        await new Promise(r => setTimeout(r, delay))
      }
    }
    console.log(`ğŸ‘· Worker ${id} finished`)
  }

  private async processUrl(url: string) {
    const strategy = this.strategies.find(s => s.match(url))
    if (!strategy) {
      console.warn(`âš ï¸  No strategy for ${url}, skipping.`)
      return
    }

    const page = await this.browser!.newPage()
    page.on('console', msg => console.log('PAGE LOG:', msg.text()))
    try {
      const isBookList = url.includes('booklist') || url.includes('/list/')
      const isChapter = url.includes('/chapter/') || url.includes('/read/')
      const isManga = url.includes('/book/') || url.includes('/manhua/')

      if (isBookList && strategy.getMangaList) {
        console.log('ğŸ“‹ Detected List Page. Discovering...')
        const { mangas, next } = await strategy.getMangaList(url, page)

        const fullMangaUrls = mangas.map(u => u.startsWith('http') ? u : `${strategy.baseUrl}${u}`)
        fullMangaUrls.forEach((u) => {
          if (!this.visited.has(u))
            this.queue.push(u)
        })
        console.log(`  + Discovered ${mangas.length} mangas`)

        if (next) {
          const nextUrl = next.startsWith('http') ? next : `${strategy.baseUrl}${next}`
          if (!this.visited.has(nextUrl)) {
            this.queue.push(nextUrl)
            console.log(`  + Next Page found: ${nextUrl}`)
          }
        }
      }
      else if (isChapter) {
        console.log('ğŸ“– Detected Chapter Page. Fetching content...')
        const content = await strategy.getChapterContent(url, page)

        // Check if chapter already exists and is complete
        if (content.comicSlug && content.chapterSlug) {
          try {
            const status: any = await this.syncToApi('/api/admin/check-chapter', null, {
              method: 'GET',
              searchParams: { comicSlug: content.comicSlug, chapterSlug: content.chapterSlug },
            })

            if (status && status.exists && status.count > 0 && !status.hasFailures) {
              // Allow small variance in page count
              if (Math.abs(status.count - content.images.length) <= 5) {
                console.log(`  â­ï¸  Skipping existing chapter: ${content.title} (${status.count} images)`)
                return
              }
              console.log(`  âš ï¸  Chapter update detected (DB: ${status.count}, Site: ${content.images.length}). Re-crawling...`)
            }
          }
          catch {
            console.warn('  âš ï¸  Failed to check chapter status, proceeding with crawl.')
          }
        }

        if (content.images.length === 0) {
          console.warn(`âš ï¸  No images found for ${url}. Dumping HTML...`)
          const fs = await import('node:fs/promises')
          const path = await import('node:path')
          const dumpPath = path.resolve('chapter_content.html')
          await fs.writeFile(dumpPath, await page.content())
          console.log(`Saved HTML to ${dumpPath}`)
        }

        if (content.images.length > 0) {
          console.log(`  Processing ${content.images.length} images...`)

          // ä¼˜åŒ–é…ç½®
          const CONCURRENCY = 10
          const MAX_RETRIES = 3
          const FAILED_IMAGE_PLACEHOLDER = 'https://placehold.co/600x800?text=Image+Load+Failed'

          const total = content.images.length
          let completed = 0

          // å¹¶å‘æ§åˆ¶å¤„ç†å‡½æ•°
          const processImage = async (imgUrl: string, globalIdx: number) => {
            const filename = String(globalIdx + 1).padStart(3, '0')
            const prefix = `comics/${content.comicSlug}/${content.chapterSlug}`

            let lastError
            for (let attempt = 1; attempt <= MAX_RETRIES; attempt++) {
              try {
                // å¼ºè¶…æ—¶æ§åˆ¶ï¼šæ¯å¼ å›¾æœ€å¤šç»™ 20 ç§’
                const processPromise = this.imageProcessor.process(imgUrl, prefix, filename)
                const timeoutPromise = new Promise((_, reject) =>
                  setTimeout(() => reject(new Error('Process timeout')), 20000),
                )

                const processed = await Promise.race([processPromise, timeoutPromise]) as any

                const selected = processed.find((p: any) => p.variant === 'preview') || processed.find((p: any) => p.variant === 'original')
                return selected?.url || FAILED_IMAGE_PLACEHOLDER
              }
              catch (e) {
                lastError = e
                // æ‰“å°é‡è¯•æ—¥å¿—ï¼Œä½†ä¸æ¢è¡Œç ´åè¿›åº¦æ¡ï¼ˆå¦‚æœå¯èƒ½ï¼‰ï¼Œæˆ–è€…ç®€å•æ¢è¡Œ
                process.stdout.write(`\n    âš ï¸  Retry ${attempt}/${MAX_RETRIES} for image ${globalIdx + 1}: ${e instanceof Error ? e.message : String(e)}\n`)
                if (attempt < MAX_RETRIES) {
                  // ç®€å•é€€é¿: 1s, 2s, ...
                  await new Promise(r => setTimeout(r, attempt * 1000))
                }
              }
            }
            console.error(`  âŒ Failed to process image ${globalIdx + 1}/${total} after ${MAX_RETRIES} attempts: ${imgUrl}`, lastError)
            return FAILED_IMAGE_PLACEHOLDER
          }
          // ä½¿ç”¨åˆ†å—å¹¶å‘ (Chunked Concurrency) - ç®€å•ä¸”æœ‰æ•ˆ
          // ä¸ºäº†é¿å… Promise.all ä¸€æ¬¡æ€§åŠ è½½å¤ªå¤šå¯¼è‡´å†…å­˜çˆ†æ¶¨ï¼Œæˆ‘ä»¬è¿˜æ˜¯åˆ†å—ï¼Œä½†å—å¤§ä¸€ç‚¹
          const processedUrls: string[] = Array.from({ length: total }, () => '')

          for (let i = 0; i < total; i += CONCURRENCY) {
            const chunk = content.images.slice(i, i + CONCURRENCY)
            const chunkPromises = chunk.map((imgUrl, idx) => {
              const globalIdx = i + idx
              return processImage(imgUrl, globalIdx).then((url) => {
                processedUrls[globalIdx] = url
                completed++
                // ç®€å•çš„è¿›åº¦æ‰“å°
                if (completed % 5 === 0 || completed === total) {
                  process.stdout.write(`\r  â³ Progress: ${completed}/${total} (${Math.round(completed / total * 100)}%)`)
                }
              })
            })

            await Promise.all(chunkPromises)
          }

          console.log('\n  âœ… Image processing complete.')

          if (processedUrls.length > 0) {
            await this.syncToApi('/api/admin/sync', {
              type: 'chapter',
              data: {
                comicSlug: content.comicSlug,
                chapterSlug: content.chapterSlug,
                title: content.title,
                images: processedUrls,
              },
            })
          }
        }
      }
      else if (isManga) {
        console.log('ğŸ“š Detected Manga Page. Syncing info...')
        const info = await strategy.getMangaInfo(url, page)

        info.chapters = info.chapters
          .map(c => ({
            ...c,
            url: c.url.startsWith('http') ? c.url : `${strategy.baseUrl}${c.url}`,
          }))
          .filter(c => c.title && c.slug && c.url)

        if (info.title && info.slug) {
          console.log(`  Syncing ${info.title} (${info.chapters.length} chapters)...`)
          await this.syncToApi('/api/admin/sync', { type: 'manga', data: info })

          // Enqueue chapters for processing
          let addedCount = 0
          for (const chapter of info.chapters) {
            const chapterUrl = chapter.url.startsWith('http') ? chapter.url : `${strategy.baseUrl}${chapter.url}`
            // Optional: Check constraint (e.g. only latest 5 if full scan to save time?)
            // For now, enqueue all. The check-chapter logic in isChapter will skip existing ones.
            if (!this.visited.has(chapterUrl)) {
              this.queue.push(chapterUrl)
              addedCount++
            }
          }
          console.log(`  + Enqueued ${addedCount} chapters for ${info.title}`)
        }
      }
    }
    finally {
      await page.close()
    }
  }
}

async function main() {
  // æ ¡éªŒç¯å¢ƒå˜é‡
  const requiredEnv = ['CLOUDFLARE_ACCOUNT_ID', 'R2_ACCESS_KEY_ID', 'R2_SECRET_ACCESS_KEY', 'CRAWLER_SECRET']
  const missing = requiredEnv.filter(k => !process.env[k])

  if (missing.length > 0) {
    console.warn(`âš ï¸  Missing environment variables: ${missing.join(', ')}`)
    // return // å…è®¸æœ¬åœ°æµ‹è¯•ç­–ç•¥æ—¶ä¸å¸¦ç¯å¢ƒå˜é‡
  }

  const crawler = new Runner({
    r2: {
      accountId: process.env.CLOUDFLARE_ACCOUNT_ID || 'mock',
      accessKeyId: process.env.R2_ACCESS_KEY_ID || 'mock',
      secretAccessKey: process.env.R2_SECRET_ACCESS_KEY || 'mock',
      bucketName: process.env.R2_BUCKET_NAME || 'starye-media',
      publicUrl: process.env.R2_PUBLIC_URL || 'http://localhost:3000',
    },
    api: {
      url: process.env.API_URL || 'http://localhost:8787',
      token: process.env.CRAWLER_SECRET || 'mock',
    },
    puppeteer: {
      executablePath: process.env.PUPPETEER_EXECUTABLE_PATH || process.env.CHROME_PATH,
    },
  })

  await crawler.run()
}

main().catch(console.error)
