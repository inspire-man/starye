name: Daily Manga Crawl

on:
  schedule:
    # 每天凌晨 2 点运行 (UTC+8 对应 UTC 18:00)
    - cron: '0 18 * * *'
  workflow_dispatch:
    inputs:
      target_url:
        description: 'Specific URL to crawl (Optional)'
        required: false
        type: string

jobs:
  crawl:
    runs-on: ubuntu-latest
    timeout-minutes: 240

    env:
      TURBO_TOKEN: ${{ secrets.TURBO_TOKEN }}
      TURBO_TEAM: ${{ secrets.TURBO_TEAM }}

    steps:
      - name: Checkout
        uses: actions/checkout@v4

      - name: Install pnpm
        uses: pnpm/action-setup@v3
        with:
          version: 10

      - name: Setup Node
        uses: actions/setup-node@v4
        with:
          node-version: 20
          cache: 'pnpm'

      - name: Install Dependencies
        run: pnpm install

      - name: Build Packages
        run: pnpm build --filter=@starye/crawler...

      - name: Run Crawler
        env:
          CLOUDFLARE_ACCOUNT_ID: ${{ secrets.CLOUDFLARE_ACCOUNT_ID }}
          R2_ACCESS_KEY_ID: ${{ secrets.R2_ACCESS_KEY_ID }}
          R2_SECRET_ACCESS_KEY: ${{ secrets.R2_SECRET_ACCESS_KEY }}
          R2_BUCKET_NAME: ${{ secrets.R2_BUCKET_NAME }}
          R2_PUBLIC_URL: ${{ secrets.R2_PUBLIC_URL }}
          CRAWLER_SECRET: ${{ secrets.CRAWLER_SECRET }}
          API_URL: ${{ secrets.API_URL }}
          # Puppeteer Cache
          PUPPETEER_SKIP_CHROMIUM_DOWNLOAD: true
        run: |
          # 动态查找 Chrome 路径
          export PUPPETEER_EXECUTABLE_PATH=$(which google-chrome-stable || which google-chrome || which chromium-browser)
          echo "Found Chrome at: $PUPPETEER_EXECUTABLE_PATH"

          # 如果有输入 URL 则使用，否则默认爬取首页（示例）
          TARGET="${{ inputs.target_url }}"
          if [ -z "$TARGET" ]; then
            TARGET="https://www.92hm.life/booklist?end=0" # 默认目标: 最新更新列表
          fi

          echo "Starting crawl for: $TARGET"

          # 使用 puppeteer-headful (xvfb) 或者直接使用 chrome-headless-shell
          # 这里假设 base-crawler 配置了自动使用 headless

          pnpm --filter @starye/crawler start "$TARGET"

      - name: Build & Upload Search Index
        if: success()
        env:
          CLOUDFLARE_ACCOUNT_ID: ${{ secrets.CLOUDFLARE_ACCOUNT_ID }}
          R2_ACCESS_KEY_ID: ${{ secrets.R2_ACCESS_KEY_ID }}
          R2_SECRET_ACCESS_KEY: ${{ secrets.R2_SECRET_ACCESS_KEY }}
          R2_BUCKET_NAME: ${{ secrets.R2_BUCKET_NAME }}
          API_URL: ${{ secrets.API_URL }}
        run: |
          echo "Building Orama Search Index..."
          pnpm --filter @starye/crawler build:search
